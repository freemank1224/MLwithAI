## 0. 导入

### 0.1 线性回归
#### 基本概念:
预测连续值线性回归就像是在散点图上画一条最佳拟合线。想象你在观察房屋面积和价格的关系,你可能会发现它们之间存在某种线性关系。线性回归就是找到这条最能代表数据趋势的直线。
#### 数学原理(简化版)
线性回归的基本形式是:
$y = wx + b$
其中:
$y$ 是预测值
$x$ 是输入特征
$w$ 是权重(斜率)
$b$ 是偏置(截距)
目标是找到最佳的$w$和$b$,使预测值与实际值的差异最小

#### 梯度下降优化
梯度下降是一种优化算法,用于找到使损失函数最小的参数。我们可以将其想象成在山地中寻找最低点的过程。

##### 直观解释:
1. 想象你站在一座山上,目标是到达山谷的最低点。
2. 你环顾四周,选择最陡峭的下坡方向。
3. 沿着这个方向走一小步。
4. 重复步骤2和3,直到你到达山谷底部。

##### 在线性回归中:
- "山"是由参数(w和b)和损失函数(通常是均方误差)形成的。
- "最陡峭的方向"由损失函数对参数的偏导数(梯度)决定。
- "小步"的大小由学习率决定。

##### 数学表示:
1. 计算损失函数对参数的梯度:
   $\nabla J(w,b) = (\frac{\partial J}{\partial w}, \frac{\partial J}{\partial b})$
2. 更新参数:
   $w = w - \alpha \frac{\partial J}{\partial w}$
   $b = b - \alpha \frac{\partial J}{\partial b}$
   其中 $\alpha$ 是学习率。

##### Python代码示例:
```python
python
import numpy as np
import matplotlib.pyplot as plt
#生成示例数据
np.random.seed(42)
X = 2 np.random.rand(100, 1)
y = 4 + 3 X + np.random.randn(100, 1)
#初始化参数
w = 0
b = 0
#设置超参数
learning_rate = 0.1
n_iterations = 1000
#存储每次迭代的参数和损失
w_history, b_history, loss_history = [], [], []
#梯度下降
for i in range(n_iterations):
# 计算预测值
y_pred = w X + b
# 计算梯度
dw = (2/100) np.sum(X (y_pred - y))
db = (2/100) np.sum(y_pred - y)
# 更新参数
w -= learning_rate dw
b -= learning_rate db
# 计算损失
loss = np.mean((y_pred - y) 2)
# 存储历史记录
w_history.append(w)
b_history.append(b)
loss_history.append(loss)
#绘制损失函数随迭代次数的变化
plt.figure(figsize=(10, 5))
plt.subplot(121)
plt.plot(loss_history)
plt.title('损失函数随迭代次数的变化')
plt.xlabel('迭代次数')
plt.ylabel('损失')
#绘制最终的拟合线
plt.subplot(122)
plt.scatter(X, y, alpha=0.5)
plt.plot(X, w X + b, color='r', linewidth=2)
plt.title('最终拟合结果')
plt.xlabel('X')
plt.ylabel('y')
plt.tight_layout()
plt.show()
print(f"最终参数: w = {w.2f}, b = {b.2f}")
```

### 0.2 逻辑回归

#### 基本概念：二分类问题

逻辑回归是一种用于解决二分类问题的算法。它试图找到一个决策边界，将数据分为两类。

#### Sigmoid函数

逻辑回归使用Sigmoid函数将线性输出转换为0到1之间的概率值：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 $z = wx + b$，$w$ 是权重向量，$x$ 是输入特征，$b$ 是偏置项。

#### 决策边界

决策边界是将两个类别分开的线（在二维情况下）或超平面（在高维情况下）。

#### 损失函数与优化

逻辑回归使用对数似然损失函数：

$J(w,b) = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$

其中 $m$ 是样本数量，$y^{(i)}$ 是真实标签，$\hat{y}^{(i)}$ 是预测概率。

#### Python代码示例：

以下是一个使用sklearn实现逻辑回归的例子，我们将创建一个简单的二分类数据集并进行分类：

### 0.2 逻辑回归

#### 基本概念：二分类问题

逻辑回归是一种用于解决二分类问题的算法。它试图找到一个决策边界，将数据分为两类。

#### Sigmoid函数

逻辑回归使用Sigmoid函数将线性输出转换为0到1之间的概率值：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 $z = wx + b$，$w$ 是权重向量，$x$ 是输入特征，$b$ 是偏置项。

#### 决策边界

决策边界是将两个类别分开的线（在二维情况下）或超平面（在高维情况下）。

#### 损失函数与优化

逻辑回归使用对数似然损失函数：

$J(w,b) = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$

其中 $m$ 是样本数量，$y^{(i)}$ 是真实标签，$\hat{y}^{(i)}$ 是预测概率。

#### Python代码示例：

以下是一个使用sklearn实现逻辑回归的例子，我们将创建一个简单的二分类数据集并进行分类：

---


## 1. 非监督学习
### 1.1 聚类
#### 1.1.1 K-means算法
算法步骤
示例:客户分群
优缺点分析
#### 1.1.2 层次聚类
自底向上vs自顶向下
距离度量方法
示例:基因表达数据聚类
### 1.2 主成分分析(PCA)
#### 降维的概念
#### 特征向量与特征值
应用:图像压缩
## 2. 监督学习
### 2.1 决策树
#### 信息增益与基尼不纯度
#### 树的生成与剪枝
示例:信用评分模型
### 2.2 随机森林
#### 集成学习思想
#### Bagging与特征随机选择
优势:抗过拟合
2.3 支持向量机(SVM)
#### 最大间隔分类器
#### 核技巧
#### 软间隔SVM
### 2.4 神经网络
#### 神经元与激活函数
#### 前馈神经网络
#### 反向传播算法
### 2.5 深度学习
#### 卷积神经网络(CNN)
对于每个主题,我们可以按以下结构展开:
1. 直观解释
数学原理(简化版)
图形化展示
简单的Python代码示例
实际应用案例
例如,对于K-means聚类,我们可以这样展开:
1.1.1 K-means聚类
K-means是一种简单而强大的聚类算法,它试图将数据点分成K个组(簇),使得每个数据点属于离它最近的簇中心。
直观解释
想象你有一堆彩色的弹珠,你想把它们分成3组。你可能会这样做:
随机选择3个位置作为初始的组中心
将每个弹珠分配到最近的中心
移动中心到每组弹珠的平均位置
重复步骤2和3,直到中心不再明显移动
算法步骤
初始化:随机选择K个数据点作为初始簇中心
2. 分配:将每个数据点分配到最近的簇中心
3. 更新:重新计算每个簇的中心(均值)
重复步骤2和3,直到簇中心基本不变或达到最大迭代次数
图形化展示
[在这里插入一个动画或多步骤图,展示K-means的迭代过程]
Python代码示例
实际应用案例
客户分群:电商平台可以使用K-means来对客户进行分群,基于他们的购买行为、浏览历史等特征。这有助于个性化营销和产品推荐。
优缺点分析
优点:
简单易懂,实现方便
对大数据集也能高效运行
缺点:
需要预先指定簇的数量K
对初始中心点的选择敏感
不适用于非球形簇
这种结构可以帮助学生从直观理解逐步深入到技术细节,同时通过代码和实际案例加深理解。我们可以为每个主题采用类似的方法。