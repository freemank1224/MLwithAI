## 0. 导入

### 0.1 线性回归
#### 基本概念:
预测连续值线性回归就像是在散点图上画一条最佳拟合线。想象你在观察房屋面积和价格的关系,你可能会发现它们之间存在某种线性关系。线性回归就是找到这条最能代表数据趋势的直线。
#### 数学原理(简化版)
线性回归的基本形式是:
$y = wx + b$
其中:
$y$ 是预测值
$x$ 是输入特征
$w$ 是权重(斜率)
$b$ 是偏置(截距)
目标是找到最佳的$w$和$b$,使预测值与实际值的差异最小

#### 梯度下降优化
梯度下降是一种优化算法,用于找到使损失函数最小的参数。我们可以将其想象成在山地中寻找最低点的过程。

##### 直观解释:
1. 想象你站在一座山上,目标是到达山谷的最低点。
2. 你环顾四周,选择最陡峭的下坡方向。
3. 沿着这个方向走一小步。
4. 重复步骤2和3,直到你到达山谷底部。

##### 在线性回归中:
- "山"是由参数(w和b)和损失函数(通常是均方误差)形成的。
- "最陡峭的方向"由损失函数对参数的偏导数(梯度)决定。
- "小步"的大小由学习率决定。

##### 数学表示:
1. 计算损失函数对参数的梯度:
   $\nabla J(w,b) = (\frac{\partial J}{\partial w}, \frac{\partial J}{\partial b})$
2. 更新参数:
   $w = w - \alpha \frac{\partial J}{\partial w}$
   $b = b - \alpha \frac{\partial J}{\partial b}$
   其中 $\alpha$ 是学习率。

##### Python代码示例:

以下是线性回归的Python代码示例的链接:

[线性回归代码示例](LinearReg.ipynb)

### 0.2 逻辑回归

#### 基本概念：二分类问题

逻辑回归是一种用于解决二分类问题的算法。它试图找到一个决策边界，将数据分为两类。

#### Sigmoid函数

逻辑回归使用Sigmoid函数将线性输出转换为0到1之间的概率值：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 $z = wx + b$，$w$ 是权重向量，$x$ 是输入特征，$b$ 是偏置项。

#### 决策边界

决策边界是将两个类别分开的线（在二维情况下）或超平面（在高维情况下）。

#### 损失函数与优化

逻辑回归使用对数似然损失函数：

$J(w,b) = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$

其中 $m$ 是样本数量，$y^{(i)}$ 是真实标签，$\hat{y}^{(i)}$ 是预测概率。

#### Python代码示例：

以下是一个使用sklearn实现逻辑回归的例子，我们将创建一个简单的二分类数据集并进行分类：

### 0.2 逻辑回归

#### 基本概念：二分类问题

逻辑回归是一种用于解决二分类问题的算法。它试图找到一个决策边界，将数据分为两类。

#### Sigmoid函数

逻辑回归使用Sigmoid函数将线性输出转换为0到1之间的概率值：

$\sigma(z) = \frac{1}{1 + e^{-z}}$

其中 $z = wx + b$，$w$ 是权重向量，$x$ 是输入特征，$b$ 是偏置项。

#### 决策边界

决策边界是将两个类别分开的线（在二维情况下）或超平面（在高维情况下）。

#### 损失函数与优化

逻辑回归使用对数似然损失函数：

$J(w,b) = -\frac{1}{m}\sum_{i=1}^m [y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})]$

其中 $m$ 是样本数量，$y^{(i)}$ 是真实标签，$\hat{y}^{(i)}$ 是预测概率。

#### Python代码示例：

以下是一个使用sklearn实现逻辑回归的例子，我们将创建一个简单的二分类数据集并进行分类：


[逻辑回归示例代码](LogisticReg.ipynb)



---


## 1. 非监督学习
### 1.1 聚类
**聚类的本质**是一种无监督的数据探索和组织方法。它通过分析数据点之间的相似性和差异性，自动发现数据集中潜在的结构和模式，将相似的对象归为一组，不同的对象分开。这个过程不需要预先标记的数据，而是直接从数据的特征中学习，从而揭示数据的内在关系，简化复杂的数据结构，并提供对数据的多层次理解。聚类既是一种数据简化和抽象的手段，也是一个信息压缩和模式识别的过程，它在数据分析、模式识别和机器学习等领域有广泛的应用。
#### 1.1.1 生活中的聚类
想象一下,当我们整理衣柜时,我们通常会不自觉地将衣物进行分类。我们可能会根据以下几个特征来对衣物进行聚类:
1. 类型:我们可能会将上衣、裤子、裙子等不同类型的衣物分开放置。
2. 季节:我们可能会将夏装、冬装分开,以便于不同季节的穿着。
3. 场合:我们可能会将正装、休闲服、运动服等分开,以适应不同的场合需求。
4. 颜色:我们可能会将相似颜色的衣物放在一起,方便搭配。
5. 材质:我们可能会将需要特殊护理的衣物(如羊毛衫、丝绸衣物)单独放置。

这个过程中,我们的大脑在没有明确指令的情况下,基于我们的经验和习惯,自然而然地对衣物进行了多维度的聚类。这种聚类方式帮助我们更有效地管理和使用我们的衣物,体现了人类在日常生活中进行无监督学习和分类的能力。

#### 1.1.2 K-means算法
算法步骤
示例:客户分群
优缺点分析
#### 1.1.3 代码示例
[K-means聚类示例代码](Kmeans.ipynb)

#### 1.1.4 层次聚类

层次聚类是另一种常用的聚类方法，它通过创建数据点的层次结构来进行聚类。这种方法不需要预先指定簇的数量，而是生成一个树状图（称为树状图或dendrogram），展示数据点如何逐步合并或分裂。

层次聚类主要有两种方法：

1. 凝聚式层次聚类（自底向上）：
   - 开始时，每个数据点被视为一个独立的簇。
   - 逐步合并最相似的簇，直到所有数据点归为一个大簇。
   - 这个过程形成了一个树状结构，展示了簇的合并历史。

2. 分裂式层次聚类（自顶向下）：
   - 开始时，所有数据点被视为一个大簇。
   - 逐步将簇分裂成更小的簇，直到每个数据点成为一个独立的簇。
   - 这个过程也形成了一个树状结构，展示了簇的分裂历史。

层次聚类的优点：
- 不需要预先指定簇的数量。
- 可以生成直观的树状图，帮助理解数据结构。
- 适用于发现数据的层次结构。

缺点：
- 计算复杂度较高，不适合大规模数据集。
- 一旦合并或分裂完成，就不能撤销，可能导致错误累积。

层次聚类在生物信息学、社会网络分析等领域有广泛应用，特别适合需要探索数据内在层次结构的场景。

示例:基因表达数据聚类






### 1.2 主成分分析(PCA)
主成分分析(PCA)是一种常用的降维技术,它可以帮助我们从复杂的高维数据中提取最重要的信息。让我们用一个简单的生活例子来理解PCA的概念和思路:

#### 1.2.1 生活中的示例
想象你是一名摄影师,正在拍摄一群人的合影。你希望在一张照片中尽可能多地展示每个人的特征。这时,你会怎么做?

1. 选择最佳角度:你会选择一个能够最大程度展示所有人特征的角度。这就像PCA找到的第一个主成分,它能够最大程度地保留原始数据的信息。

2. 调整构图:如果第一个角度还不够,你可能会再拍一张从侧面的照片,以捕捉第一张照片中看不到的特征。这就像PCA的第二个主成分,它捕捉了与第一个主成分正交(不相关)的信息。

3. 舍弃次要信息:你可能不会去专门拍摄每个人的脚,因为这些细节对于整体特征的展示贡献不大。这就像PCA舍弃那些对数据变化贡献较小的维度。

4. 压缩信息:最后,你可能只需要2-3张照片就能很好地展示这群人的整体特征,而不是为每个人单独拍摄。这就是PCA降维的本质,用较少的维度代表原始的高维数据。

通过这个例子,我们可以看到PCA的核心思想:

- 找到数据变化最大的方向(主成分)
- 用较少的维度代表原始数据,同时保留最重要的信息
- 舍弃对数据整体特征贡献较小的维度

PCA在数据可视化、特征提取、噪声消除等多个领域有广泛应用。它帮助我们从复杂的数据中提取出最本质的信息,使我们能够更好地理解和分析数据。

#### 1.2.2 降维
降维是PCA的核心操作，它就像是数据的"瘦身计划"。让我们通过一个简单的比喻来理解降维的过程和意义：

想象你有一个装满各种物品的大箱子，这个箱子就像是高维数据。你想把这些物品搬到一个更小的箱子里，但又不想丢失太多重要的东西。这就是降维要做的事情。

1. 提取关键特征：
   就像你会优先选择最重要的物品放入小箱子，PCA会找出数据中最关键的特征（主成分）。这些特征携带了最多的信息，就像是你最常用的物品。

2. 压缩信息：
   你可能会把一些相似的物品合并在一起，比如把所有的袜子卷在一起。PCA也是这样，它会将相关的特征组合成新的、更少的特征。

3. 去除冗余：
   箱子里可能有一些很少用到的物品，你可以安全地将它们留下。同样，PCA会去除那些对数据变化贡献很小的维度。

4. 保留本质：
   虽然新的小箱子装的东西少了，但最重要的物品都在里面。PCA降维后的数据虽然维度减少了，但保留了数据的核心信息。

为什么降维能帮助提取信息的本质？

1. 去除噪声：
   就像过滤掉背景噪音能让你更清晰地听到重要的声音，降维可以帮助我们去除数据中的"噪声"，突出真正重要的信息。

2. 简化复杂性：
   高维数据就像一团乱麻，降维就是找出其中的主线。这样可以让我们更容易理解数据的结构和模式。

3. 发现隐藏关系：
   有时候，重要的关系被埋藏在复杂的数据中。降维就像是一个放大镜，帮助我们发现这些隐藏的关系。

4. 提高效率：
   处理更少的维度意味着需要更少的计算资源。这就像是用一个小背包代替大行李箱，让你的旅行更加轻松高效。

通过降维，我们能够抓住数据的"灵魂"，而不是被繁杂的细节所困扰。

#### 1.2.3 代码示例

[PCA代码示例](PCA.ipynb)
---
## 2. 监督学习
### 2.1 决策树
#### 信息增益与基尼不纯度
#### 树的生成与剪枝
示例:信用评分模型
### 2.2 随机森林
#### 集成学习思想
#### Bagging与特征随机选择
优势:抗过拟合
2.3 支持向量机(SVM)
#### 最大间隔分类器
#### 核技巧
#### 软间隔SVM
### 2.4 神经网络
#### 神经元与激活函数
#### 前馈神经网络
#### 反向传播算法
### 2.5 深度学习
#### 卷积神经网络(CNN)
对于每个主题,我们可以按以下结构展开:
1. 直观解释
数学原理(简化版)
图形化展示
简单的Python代码示例
实际应用案例
例如,对于K-means聚类,我们可以这样展开:
1.1.1 K-means聚类
K-means是一种简单而强大的聚类算法,它试图将数据点分成K个组(簇),使得每个数据点属于离它最近的簇中心。
直观解释
想象你有一堆彩色的弹珠,你想把它们分成3组。你可能会这样做:
随机选择3个位置作为初始的组中心
将每个弹珠分配到最近的中心
移动中心到每组弹珠的平均位置
重复步骤2和3,直到中心不再明显移动
算法步骤
初始化:随机选择K个数据点作为初始簇中心
2. 分配:将每个数据点分配到最近的簇中心
3. 更新:重新计算每个簇的中心(均值)
重复步骤2和3,直到簇中心基本不变或达到最大迭代次数
图形化展示
[在这里插入一个动画或多步骤图,展示K-means的迭代过程]
Python代码示例
实际应用案例
客户分群:电商平台可以使用K-means来对客户进行分群,基于他们的购买行为、浏览历史等特征。这有助于个性化营销和产品推荐。
优缺点分析
优点:
简单易懂,实现方便
对大数据集也能高效运行
缺点:
需要预先指定簇的数量K
对初始中心点的选择敏感
不适用于非球形簇
这种结构可以帮助学生从直观理解逐步深入到技术细节,同时通过代码和实际案例加深理解。我们可以为每个主题采用类似的方法。